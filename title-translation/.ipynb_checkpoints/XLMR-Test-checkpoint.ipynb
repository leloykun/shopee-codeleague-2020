{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U -q transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import subprocess, gc\n\n# from google.cloud import storage\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow.keras as tfk\nfrom transformers import XLMRobertaTokenizer, TFXLMRobertaModel, TFXLMRobertaForMaskedLM\ntokenizer = XLMRobertaTokenizer.from_pretrained('jplu/tf-xlm-roberta-base')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initialize TPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Prepare utility functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set your own project id here\n# PROJECT_ID = 'runner'\n# storage_client = storage.Client(project=PROJECT_ID)\n# \n# def upload_blob(bucket_name, source_file_name, destination_blob_name):\n#     bucket = storage_client.get_bucket(bucket_name)\n#     blob = bucket.blob(destination_blob_name)\n#     blob.upload_from_filename(source_file_name)\n#     print('File {} uploaded to {}.'.format(\n#         source_file_name,\n#         destination_blob_name))\n    \ndef run_command(command):\n    process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n    return process.communicate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"run_command(\"gsutil rm -r product-translation-dataset\")\nrun_command(\"gsutil cp -r gs://shopee-title-translation/product-translation-dataset/ .\")\nrun_command(\"ls\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tcn  = pd.read_csv('product-translation-dataset/train_tcn_clean.csv')\ndf_en   = pd.read_csv('product-translation-dataset/train_en_clean.csv')\ndf_test = pd.read_csv('product-translation-dataset/test_clean.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Concatenate datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.concat([df_tcn, df_en, df_test]).reset_index().drop(['index'], axis=1)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def mlm_loss(labels, logits):\n    loss_fn = tfk.losses.SparseCategoricalCrossentropy(\n        from_logits=True, reduction=tf.keras.losses.Reduction.NONE #SUM\n    )\n    # make sure only labels that are not equal to -100\n    # are taken into account as loss\n    active_loss = tf.reshape(labels, (-1,)) != -100\n    reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, tf.shape(logits)[2])), active_loss)\n    labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)\n    return loss_fn(labels, reduced_logits)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load XLMR from huggingface","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xlmr_model = TFXLMRobertaForMaskedLM.from_pretrained('jplu/tf-xlm-roberta-base')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xlmr_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"xlmr_model.save_pretrained('xlmr_model_weights')\nrun_command(\"gsutil rm -r gs://shopee-title-translation/xlmr_model_weights\")\nrun_command(\"gsutil cp -r xlmr_model_weights gs://shopee-title-translation\")\n!ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load from saved weights","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nrun_command(\"rm -r xlmr_model_weights\")\nrun_command(\"gsutil cp -r gs://shopee-title-translation/xlmr_model_weights ./\")\nrun_command(\"ls\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xlmr_model = TFXLMRobertaForMaskedLM.from_pretrained('xlmr_model_weights')\nxlmr_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_tokens(sentences, mask_prob=0.15):\n    res = tokenizer(sentences,\n                    max_length=64,\n                    truncation=True,\n                    #padding='max_length',\n                    padding=True,\n                    return_tensors='tf',\n                    return_attention_mask=True,\n                    return_special_tokens_mask=True)\n    input_tokens        = res['input_ids']\n    attention_mask      = res['attention_mask']\n    special_tokens_mask = res['special_tokens_mask']\n    \n    mask = tf.cast(tf.random.uniform(shape=tf.shape(input_tokens)) < mask_prob, 'int32') * (1 - special_tokens_mask)\n    \n    masked_input_tokens = input_tokens * (1-mask) + 250001 * mask\n    label_input_tokens  = input_tokens * mask + -100 * (1-mask)\n    return masked_input_tokens, label_input_tokens, attention_mask\n\ndef generate_data(df_train, batch_size, mask_prob=0.3):\n    df_train = df_train.sample(frac=1.0)\n    for i in range(0, len(df_train)-batch_size, batch_size):\n        X, Y, attention_mask = calc_tokens(list(df_train['product_title'].iloc[i:i+batch_size].to_numpy()), mask_prob)\n        yield {'input_ids': X, 'labels':Y, 'attention_mask': attention_mask}, Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='loss',\n    factor=0.2,\n    patience=20,\n    min_lr=1e-7\n)\n\nearly_stop = tfk.callbacks.EarlyStopping(\n    monitor='loss',\n    patience=60,\n    restore_best_weights=True\n)\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\n    log_dir=\"logs\",\n    histogram_freq=0,\n    update_freq=100,\n    write_graph=False,\n    profile_batch = 0\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xlmr_model.compile(optimizer=tfk.optimizers.Adam(learning_rate=1e-5),\n                   loss=mlm_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"histories = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for v in range(19, 30+1):\n    print(\"---------------- {} ----------------\".format(v))\n    print(\"Learning rate: {:.2E}\".format(tfk.backend.get_value(getattr(xlmr_model.optimizer, \"lr\", None))))\n    history = xlmr_model.fit(generate_data(df_train, 32, 0.3),\n                             steps_per_epoch=200,\n                             epochs=30,\n                             callbacks=[\n                                 reduce_lr,\n                                 early_stop,\n                                 tensorboard_callback\n                             ]\n                            )\n    histories.append(pd.DataFrame(history.history))\n    pd.concat(histories).to_csv(\"history.csv\".format(v), index=False)\n    run_command(\"gsutil rm gs://shopee-title-translation/history.csv\")\n    run_command(\"gsutil cp history.csv gs://shopee-title-translation\")\n    \n    run_command(\"rm -r xlmr_model_weights\")\n    xlmr_model.save_pretrained('xlmr_model_weights')\n    run_command(\"gsutil rm -r gs://shopee-title-translation/xlmr_model_weights\")\n    run_command(\"gsutil cp -r xlmr_model_weights gs://shopee-title-translation\")\n    \n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_sentences = [\"Recollections Color Splash Clear Stamps & Stencil\",\n                   \"̅卡通辛普森兒童衛衣圓領男潮童洋氣小寶寶女童卡通春加絨上衣\",\n                   \"美軍半指手套cs戰術防軍工攀登部隊式軍事軍版野外加厚07防寒防滑\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"masked_input_tokens, label_input_tokens, attention_mask = calc_tokens(input_sentences)\npredictions = xlmr_model({'input_ids': masked_input_tokens, 'labels': label_input_tokens, 'attention_mask': attention_mask})[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"masked_sentences = [tokenizer.decode(tokens) for tokens in masked_input_tokens]\nmasked_sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reconstructed_sentences = [tokenizer.decode(tokens) for tokens in predictions.numpy().argmax(axis=2)]\nreconstructed_sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlm_loss(label_input_tokens, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}