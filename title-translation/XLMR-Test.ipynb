{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, gc\n",
    "\n",
    "# from google.cloud import storage\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "from transformers import XLMRobertaTokenizer, TFXLMRobertaModel, TFXLMRobertaForMaskedLM\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('jplu/tf-xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "# tf.config.experimental_connect_to_cluster(tpu)\n",
    "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your own project id here\n",
    "# PROJECT_ID = 'runner'\n",
    "# storage_client = storage.Client(project=PROJECT_ID)\n",
    "# \n",
    "# def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "#     bucket = storage_client.get_bucket(bucket_name)\n",
    "#     blob = bucket.blob(destination_blob_name)\n",
    "#     blob.upload_from_filename(source_file_name)\n",
    "#     print('File {} uploaded to {}.'.format(\n",
    "#         source_file_name,\n",
    "#         destination_blob_name))\n",
    "    \n",
    "def run_command(command):\n",
    "    process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n",
    "    return process.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command(\"gsutil rm -r product-translation-dataset\")\n",
    "run_command(\"gsutil cp -r gs://shopee-title-translation/product-translation-dataset/ .\")\n",
    "run_command(\"ls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tcn  = pd.read_csv('product-translation-dataset/train_tcn_clean.csv')\n",
    "df_en   = pd.read_csv('product-translation-dataset/train_en_clean.csv')\n",
    "df_test = pd.read_csv('product-translation-dataset/test_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_tcn, df_en, df_test]).reset_index().drop(['index'], axis=1)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_loss(labels, logits):\n",
    "    loss_fn = tfk.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE #SUM\n",
    "    )\n",
    "    # make sure only labels that are not equal to -100\n",
    "    # are taken into account as loss\n",
    "    active_loss = tf.reshape(labels, (-1,)) != -100\n",
    "    reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, tf.shape(logits)[2])), active_loss)\n",
    "    labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)\n",
    "    return loss_fn(labels, reduced_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load XLMR from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_model = TFXLMRobertaForMaskedLM.from_pretrained('jplu/tf-xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "xlmr_model.save_pretrained('xlmr_model_weights')\n",
    "run_command(\"gsutil rm -r gs://shopee-title-translation/xlmr_model_weights\")\n",
    "run_command(\"gsutil cp -r xlmr_model_weights gs://shopee-title-translation\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_command(\"rm -r xlmr_model_weights\")\n",
    "run_command(\"gsutil cp -r gs://shopee-title-translation/xlmr_model_weights ./\")\n",
    "run_command(\"ls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_model = TFXLMRobertaForMaskedLM.from_pretrained('xlmr_model_weights')\n",
    "xlmr_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tokens(sentences, mask_prob=0.15):\n",
    "    res = tokenizer(sentences,\n",
    "                    max_length=64,\n",
    "                    truncation=True,\n",
    "                    #padding='max_length',\n",
    "                    padding=True,\n",
    "                    return_tensors='tf',\n",
    "                    return_attention_mask=True,\n",
    "                    return_special_tokens_mask=True)\n",
    "    input_tokens        = res['input_ids']\n",
    "    attention_mask      = res['attention_mask']\n",
    "    special_tokens_mask = res['special_tokens_mask']\n",
    "    \n",
    "    mask = tf.cast(tf.random.uniform(shape=tf.shape(input_tokens)) < mask_prob, 'int32') * (1 - special_tokens_mask)\n",
    "    \n",
    "    masked_input_tokens = input_tokens * (1-mask) + 250001 * mask\n",
    "    label_input_tokens  = input_tokens * mask + -100 * (1-mask)\n",
    "    return masked_input_tokens, label_input_tokens, attention_mask\n",
    "\n",
    "def generate_data(df_train, batch_size, mask_prob=0.3):\n",
    "    df_train = df_train.sample(frac=1.0)\n",
    "    for i in range(0, len(df_train)-batch_size, batch_size):\n",
    "        X, Y, attention_mask = calc_tokens(list(df_train['product_title'].iloc[i:i+batch_size].to_numpy()), mask_prob)\n",
    "        yield {'input_ids': X, 'labels':Y, 'attention_mask': attention_mask}, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss',\n",
    "    factor=0.2,\n",
    "    patience=20,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "early_stop = tfk.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=60,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=\"logs\",\n",
    "    histogram_freq=0,\n",
    "    update_freq=100,\n",
    "    write_graph=False,\n",
    "    profile_batch = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_model.compile(optimizer=tfk.optimizers.Adam(learning_rate=1e-5),\n",
    "                   loss=mlm_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in range(19, 30+1):\n",
    "    print(\"---------------- {} ----------------\".format(v))\n",
    "    print(\"Learning rate: {:.2E}\".format(tfk.backend.get_value(getattr(xlmr_model.optimizer, \"lr\", None))))\n",
    "    history = xlmr_model.fit(generate_data(df_train, 32, 0.3),\n",
    "                             steps_per_epoch=200,\n",
    "                             epochs=30,\n",
    "                             callbacks=[\n",
    "                                 reduce_lr,\n",
    "                                 early_stop,\n",
    "                                 tensorboard_callback\n",
    "                             ]\n",
    "                            )\n",
    "    histories.append(pd.DataFrame(history.history))\n",
    "    pd.concat(histories).to_csv(\"history.csv\".format(v), index=False)\n",
    "    run_command(\"gsutil rm gs://shopee-title-translation/history.csv\")\n",
    "    run_command(\"gsutil cp history.csv gs://shopee-title-translation\")\n",
    "    \n",
    "    run_command(\"rm -r xlmr_model_weights\")\n",
    "    xlmr_model.save_pretrained('xlmr_model_weights')\n",
    "    run_command(\"gsutil rm -r gs://shopee-title-translation/xlmr_model_weights\")\n",
    "    run_command(\"gsutil cp -r xlmr_model_weights gs://shopee-title-translation\")\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = [\"Recollections Color Splash Clear Stamps & Stencil\",\n",
    "                   \"̅卡通辛普森兒童衛衣圓領男潮童洋氣小寶寶女童卡通春加絨上衣\",\n",
    "                   \"美軍半指手套cs戰術防軍工攀登部隊式軍事軍版野外加厚07防寒防滑\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_input_tokens, label_input_tokens, attention_mask = calc_tokens(input_sentences)\n",
    "predictions = xlmr_model({'input_ids': masked_input_tokens, 'labels': label_input_tokens, 'attention_mask': attention_mask})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sentences = [tokenizer.decode(tokens) for tokens in masked_input_tokens]\n",
    "masked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_sentences = [tokenizer.decode(tokens) for tokens in predictions.numpy().argmax(axis=2)]\n",
    "reconstructed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_loss(label_input_tokens, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
