{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q -U transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import subprocess, gc, emoji, re\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow.keras as tfk\nfrom transformers import XLMRobertaTokenizer, XLMRobertaConfig, TFXLMRobertaModel, TFXLMRobertaForMaskedLM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_command(command):\n    process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n    return process.communicate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Load dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"run_command(\"gsutil rm -r product-translation-dataset\")\nrun_command(\"gsutil cp -r gs://shopee-title-translation/product-translation-dataset .\")\nrun_command(\"gsutil cp gs://shopee-title-translation/uniq_tokens_masks.csv .\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = pd.read_csv('product-translation-dataset/all_clean.csv')\ndf_all['product_title'] = df_all['product_title'].str.lower()\ndf_all.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Initialize tokenizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = XLMRobertaTokenizer.from_pretrained('jplu/tf-xlm-roberta-base')\nspecial_tokens = [tokenizer.bos_token_id, \n                  tokenizer.eos_token_id, \n                  tokenizer.sep_token_id, \n                  tokenizer.pad_token_id, \n                  tokenizer.cls_token_id,\n                  tokenizer.mask_token_id]\nspecial_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = pd.read_csv('uniq_tokens_masks.csv')['tcn_en_mask'].to_numpy()\nfor special_token in special_tokens:\n    mask[special_token] = 1\nenc_mapping = {}\ndec_mapping = {}\nfor i in range(len(mask)):\n    if mask[i] == 0:  continue\n    enc_mapping[i] = len(enc_mapping)\n    dec_mapping[len(dec_mapping)] = i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc_table = tf.lookup.StaticVocabularyTable(\n            tf.lookup.KeyValueTensorInitializer(\n                list(enc_mapping.keys()),\n                list(enc_mapping.values()),\n                key_dtype=tf.int64,\n                value_dtype=tf.int64,\n            ),\n            num_oov_buckets=1,\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dec_table = tf.lookup.StaticVocabularyTable(\n            tf.lookup.KeyValueTensorInitializer(\n                list(dec_mapping.keys()),\n                list(dec_mapping.values()),\n                key_dtype=tf.int64,\n                value_dtype=tf.int64,\n            ),\n            num_oov_buckets=1,\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomTokenizer(XLMRobertaTokenizer):\n    def __init__(self, enc_table, dec_table, **kwargs):\n        self.enc_table = enc_table\n        self.dec_table = dec_table\n        super().__init__(**kwargs)\n    \n    def __call__(self, text, **kwargs):\n        res = super().__call__(text, **kwargs)\n        res['input_ids'] = tf.cast(enc_table.lookup(tf.cast(res['input_ids'], tf.int64)), tf.int32)\n        return res\n    def decode(self, token_ids, **kwargs):\n        token_ids = tf.cast(dec_table.lookup(tf.cast(token_ids, tf.int64)), tf.int32)\n        return super().decode(token_ids, **kwargs)\n\ntokenizer = CustomTokenizer.from_pretrained('jplu/tf-xlm-roberta-base', enc_table=enc_table, dec_table=dec_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_tokens(sentences, mask_prob=0.15, mask_id=250001):\n    res = tokenizer(sentences,\n                    max_length=64,\n                    truncation=True,\n                    #padding='max_length',\n                    padding=True,\n                    return_tensors='tf',\n                    return_attention_mask=True,\n                    return_special_tokens_mask=True)\n    input_tokens        = res['input_ids']\n    attention_mask      = res['attention_mask']\n    special_tokens_mask = res['special_tokens_mask']\n    \n    mask = tf.cast(tf.random.uniform(shape=tf.shape(input_tokens)) < mask_prob, 'int32') * (1 - special_tokens_mask)\n    \n    masked_input_tokens = input_tokens * (1-mask) + mask_id * mask\n    label_input_tokens  = input_tokens * mask + -100 * (1-mask)\n    return masked_input_tokens, label_input_tokens, attention_mask\n\ndef generate_data(df_train, batch_size, mask_prob=0.3):\n    while True:\n        df_train = df_train.sample(frac=1.0)\n        for i in range(0, len(df_train)-batch_size, batch_size):\n            X, Y, attention_mask = calc_tokens(list(df_train['product_title'].iloc[i:i+batch_size].to_numpy()), mask_prob, enc_mapping[250001])\n            yield {'input_ids': X,\n                   'labels':Y,\n                   'attention_mask': attention_mask}, Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test, Y_test = next(generate_data(df_all, batch_size=3, mask_prob=0.15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[tokenizer.decode(X_test['input_ids'][i]) for i in range(3)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initialize configuration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"config = XLMRobertaConfig(vocab_size=44512,\n                          hidden_size=256,\n                          num_hidden_layers=8,\n                          num_attention_heads=4,\n                          intermediate_size=1024,\n                          hidden_act='gelu',\n                          hidden_dropout_prob=0.1,\n                          attention_probs_dropout_prob=0.1,\n                          max_position_embeddings=68,\n                          type_vocab_size=2,\n                          initializer_range=0.02,\n                          layer_norm_eps=1e-12,\n                          gradient_checkpointing=False\n                         )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids      = tfk.layers.Input(shape=(None,), name='input_ids', dtype='int32')\nattention_mask = tfk.layers.Input(shape=(None,), name='attention_mask', dtype='int32')\nx = TFXLMRobertaForMaskedLM(config)([input_ids, attention_mask])[0]\nmodel = tfk.models.Model(inputs=[input_ids, attention_mask], outputs=x)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfk.utils.plot_model(model, show_shapes=True, dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mlm_loss(labels, logits):\n    loss_fn = tfk.losses.SparseCategoricalCrossentropy(\n        from_logits=True, reduction=tf.keras.losses.Reduction.NONE #SUM\n    )\n    # make sure only labels that are not equal to -100\n    # are taken into account as loss\n    active_loss = tf.reshape(labels, (-1,)) != -100\n    reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, tf.shape(logits)[2])), active_loss)\n    labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)\n    return loss_fn(labels, reduced_logits)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def cosine_decay_with_warmup(global_step,\n                             learning_rate_base,\n                             total_steps,\n                             warmup_learning_rate=0.0,\n                             warmup_steps=0,\n                             hold_base_rate_steps=0):\n    \"\"\"Cosine decay schedule with warm up period.\n    Cosine annealing learning rate as described in:\n      Loshchilov and Hutter, SGDR: Stochastic Gradient Descent with Warm Restarts.\n      ICLR 2017. https://arxiv.org/abs/1608.03983\n    In this schedule, the learning rate grows linearly from warmup_learning_rate\n    to learning_rate_base for warmup_steps, then transitions to a cosine decay\n    schedule.\n    Arguments:\n        global_step {int} -- global step.\n        learning_rate_base {float} -- base learning rate.\n        total_steps {int} -- total number of training steps.\n    Keyword Arguments:\n        warmup_learning_rate {float} -- initial learning rate for warm up. (default: {0.0})\n        warmup_steps {int} -- number of warmup steps. (default: {0})\n        hold_base_rate_steps {int} -- Optional number of steps to hold base learning rate\n                                    before decaying. (default: {0})\n    Returns:\n      a float representing learning rate.\n    Raises:\n      ValueError: if warmup_learning_rate is larger than learning_rate_base,\n        or if warmup_steps is larger than total_steps.\n    \"\"\"\n\n    if total_steps < warmup_steps:\n        raise ValueError('total_steps must be larger or equal to '\n                         'warmup_steps.')\n    learning_rate = 0.5 * learning_rate_base * (1 + np.cos(\n        np.pi *\n        (global_step - warmup_steps - hold_base_rate_steps\n         ) / float(total_steps - warmup_steps - hold_base_rate_steps)))\n    if hold_base_rate_steps > 0:\n        learning_rate = np.where(global_step > warmup_steps + hold_base_rate_steps,\n                                 learning_rate, learning_rate_base)\n    if warmup_steps > 0:\n        if learning_rate_base < warmup_learning_rate:\n            raise ValueError('learning_rate_base must be larger or equal to '\n                             'warmup_learning_rate.')\n        slope = (learning_rate_base - warmup_learning_rate) / warmup_steps\n        warmup_rate = slope * global_step + warmup_learning_rate\n        learning_rate = np.where(global_step < warmup_steps, warmup_rate,\n                                 learning_rate)\n    return np.where(global_step > total_steps, 0.0, learning_rate)\n\n\nclass WarmUpCosineDecayScheduler(tfk.callbacks.Callback):\n    \"\"\"Cosine decay with warmup learning rate scheduler\n    \"\"\"\n\n    def __init__(self,\n                 learning_rate_base,\n                 total_steps,\n                 global_step_init=0,\n                 warmup_learning_rate=0.0,\n                 warmup_steps=0,\n                 hold_base_rate_steps=0,\n                 verbose=0):\n        \"\"\"Constructor for cosine decay with warmup learning rate scheduler.\n    Arguments:\n        learning_rate_base {float} -- base learning rate.\n        total_steps {int} -- total number of training steps.\n    Keyword Arguments:\n        global_step_init {int} -- initial global step, e.g. from previous checkpoint.\n        warmup_learning_rate {float} -- initial learning rate for warm up. (default: {0.0})\n        warmup_steps {int} -- number of warmup steps. (default: {0})\n        hold_base_rate_steps {int} -- Optional number of steps to hold base learning rate\n                                    before decaying. (default: {0})\n        verbose {int} -- 0: quiet, 1: update messages. (default: {0})\n        \"\"\"\n\n        super(WarmUpCosineDecayScheduler, self).__init__()\n        self.learning_rate_base = learning_rate_base\n        self.total_steps = total_steps\n        self.global_step = global_step_init\n        self.warmup_learning_rate = warmup_learning_rate\n        self.warmup_steps = warmup_steps\n        self.hold_base_rate_steps = hold_base_rate_steps\n        self.verbose = verbose\n        self.learning_rates = []\n\n    def on_batch_end(self, batch, logs=None):\n        self.global_step = self.global_step + 1\n        lr = tfk.backend.get_value(self.model.optimizer.lr)\n        self.learning_rates.append(lr)\n\n    def on_batch_begin(self, batch, logs=None):\n        lr = cosine_decay_with_warmup(global_step=self.global_step,\n                                      learning_rate_base=self.learning_rate_base,\n                                      total_steps=self.total_steps,\n                                      warmup_learning_rate=self.warmup_learning_rate,\n                                      warmup_steps=self.warmup_steps,\n                                      hold_base_rate_steps=self.hold_base_rate_steps)\n        tfk.backend.set_value(self.model.optimizer.lr, lr)\n        if self.verbose > 0:\n            print('\\nBatch %05d: setting learning '\n                  'rate to %s.' % (self.global_step + 1, lr))\n\nwarm_up_lr = WarmUpCosineDecayScheduler(learning_rate_base=1e-4,\n                                        total_steps=24*120*200,  # 8 hours * 120 epochs/hour * 200 steps/epoch\n                                        warmup_learning_rate=1e-7,\n                                        warmup_steps=200,\n                                        hold_base_rate_steps=200,\n                                        global_step_init=267213\n                                       )\n            \nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='loss',\n    factor=0.2,\n    patience=90,\n    min_lr=1e-7\n)\n\nearly_stop = tfk.callbacks.EarlyStopping(\n    monitor='loss',\n    patience=180,\n    restore_best_weights=True\n)\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\n    log_dir=\"logs\",\n    histogram_freq=0,\n    update_freq=100,\n    write_graph=False,\n    profile_batch = 0\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_command(\"gsutil rm -r tcn-en-model-weights-2\")\nrun_command(\"gsutil cp -r gs://shopee-title-translation/models/tcn-en-model-weights-2 .\")\nmodel.load_weights('tcn-en-model-weights-2/tcn_to_en_model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=tfk.optimizers.Adam(),\n              loss=mlm_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model.save_weights('tcn-en-model-weights-2/tcn_to_en_model')\nrun_command(\"gsutil rm -r gs://shopee-title-translation/models/tcn-en-model-weights-2\")\nrun_command(\"gsutil cp -r tcn-en-model-weights-2 gs://shopee-title-translation/models\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for v in range(0, 50+1):\n    print(\"---------------- {} ----------------\".format(v))\n    model.fit(generate_data(df_all, batch_size=256, mask_prob=0.15),\n              steps_per_epoch=400,\n              epochs=30,\n              callbacks=[\n                reduce_lr,\n                early_stop,\n                warm_up_lr,\n                #tensorboard_callback\n              ])\n    model.save_weights('tcn-en-model-weights-2/tcn_to_en_model')\n    run_command(\"gsutil rm -r gs://shopee-title-translation/models/tcn-en-model-weights-2\")\n    run_command(\"gsutil cp -r tcn-en-model-weights-2 gs://shopee-title-translation/models\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warm_up_lr.global_step","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y = next(generate_data(df_all, 2, 0.15))\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.decode(X['input_ids'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = model.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res.argmax(axis=2)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"          0,  3215,  1336,   479, 40612, 27595, 44509,  5545, 13007,\n      44509,  9796, 27709,  9796, 21930, 41102,  9796,   819, 44509,\n       6723,   777,  5545, 26731, 33191,  9796,     6, 44509,  2288,\n       2288,     2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.decode(res.argmax(axis=2)[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}