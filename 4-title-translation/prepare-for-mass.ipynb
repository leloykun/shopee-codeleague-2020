{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q opencc emoji sklearn regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, gc, emoji, re, regex\n",
    "\n",
    "import opencc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s2t.json Simplified Chinese to Traditional Chinese 簡體到繁體\n",
    "#t2s.json Traditional Chinese to Simplified Chinese 繁體到簡體\n",
    "#s2tw.json Simplified Chinese to Traditional Chinese (Taiwan Standard) 簡體到臺灣正體\n",
    "#tw2s.json Traditional Chinese (Taiwan Standard) to Simplified Chinese 臺灣正體到簡體\n",
    "#s2hk.json Simplified Chinese to Traditional Chinese (Hong Kong variant) 簡體到香港繁體\n",
    "#hk2s.json Traditional Chinese (Hong Kong variant) to Simplified Chinese 香港繁體到簡體\n",
    "#s2twp.json Simplified Chinese to Traditional Chinese (Taiwan Standard) with Taiwanese idiom 簡體到繁體（臺灣正體標準）並轉換爲臺灣常用詞彙\n",
    "#tw2sp.json Traditional Chinese (Taiwan Standard) to Simplified Chinese with Mainland Chinese idiom 繁體（臺灣正體標準）到簡體並轉換爲中國大陸常用詞彙\n",
    "#t2tw.json Traditional Chinese (OpenCC Standard) to Taiwan Standard 繁體（OpenCC 標準）到臺灣正體\n",
    "#hk2t.json Traditional Chinese (Hong Kong variant) to Traditional Chinese 香港繁體到繁體（OpenCC 標準）\n",
    "#t2hk.json Traditional Chinese (OpenCC Standard) to Hong Kong variant 繁體（OpenCC 標準）到香港繁體\n",
    "#t2jp.json Traditional Chinese Characters (Kyūjitai) to New Japanese Kanji (Shinjitai) 繁體（OpenCC 標準，舊字體）到日文新字體\n",
    "#jp2t.json New Japanese Kanji (Shinjitai) to Traditional Chinese Characters (Kyūjitai) 日文新字體到繁體（OpenCC 標準，舊字體）\n",
    "#tw2t.json Traditional Chinese (Taiwan standard) to Traditional Chinese 臺灣正體到繁體（OpenCC 標準）\n",
    "converter_tw2s = opencc.OpenCC('tw2s.json')\n",
    "converter_t2s = opencc.OpenCC('t2s.json')\n",
    "converter_hk2s = opencc.OpenCC('hk2s.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean up datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p = emoji.get_emoji_regexp()\n",
    "p = regex.compile(r'\\p{So}')\n",
    "\n",
    "def load(filename):\n",
    "    return pd.read_csv(filename).fillna('')\n",
    "\n",
    "def preprocess(df_, apply_chinese_simplification=True):\n",
    "    df = df_[[df_.columns[0]]].copy()\n",
    "    df.columns = ['product_title']\n",
    "    df['product_title'] = df['product_title'].str.replace('\\n', ' ')\n",
    "    df['product_title'] = df['product_title'].str.replace('\\\"', ' ')\n",
    "    df['product_title'] = df['product_title'].str.replace(',', ' ')\n",
    "    #df['product_title'] = [re.sub(p, r\"\", x) for x in df['product_title'].tolist()]\n",
    "    df['product_title'] = [p.sub(\" \", x) for x in df['product_title'].tolist()]\n",
    "    if apply_chinese_simplification:\n",
    "        df['product_title'] = df['product_title']\\\n",
    "                              .apply(converter_tw2s.convert)\\\n",
    "                              .apply(converter_t2s.convert)\\\n",
    "                              .apply(converter_hk2s.convert)\n",
    "    return df\n",
    "\n",
    "def load_and_preprocess(filename, apply_chinese_simplification=True):\n",
    "    print(\"loading and processing {}...\".format(filename))\n",
    "    return preprocess(load(filename), apply_chinese_simplification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and processing product-translation-dataset/train_tcn.csv...\n",
      "loading and processing product-translation-dataset/train_en.csv...\n",
      "loading and processing product-translation-dataset/test_tcn.csv...\n",
      "loading and processing product-translation-dataset/translations/test_en.csv...\n",
      "loading and processing product-translation-dataset/dev_tcn.csv...\n",
      "loading and processing product-translation-dataset/dev_en.csv...\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filenames = ['product-translation-dataset/train_tcn.csv',\n",
    "             'product-translation-dataset/train_en.csv',\n",
    "             'product-translation-dataset/test_tcn.csv',\n",
    "             'product-translation-dataset/translations/test_en.csv',\n",
    "             'product-translation-dataset/dev_tcn.csv',\n",
    "             'product-translation-dataset/dev_en.csv']\n",
    "df_train_tcn, df_train_en, df_test_tcn, df_test_en, df_dev_tcn, df_dev_en = [load_and_preprocess(f, True) for f in filenames]\n",
    "# df_train_tcn, df_train_en, df_test_tcn, df_dev_tcn, df_dev_en = [load_and_preprocess(f, True) for f in filenames]\n",
    "# df_train_tcn, df_train_en, df_test_tcn, df_dev_tcn, df_dev_en = map(load_and_preprocess, filenames)\n",
    "# df_dev_tcn, df_dev_en = map(load_and_preprocess, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tcn2en = pd.read_csv('product-translation-dataset/translations/train_tcn2en.csv')\n",
    "df_en2tcn = pd.read_csv('product-translation-dataset/translations/train_en2tcn.csv')\n",
    "\n",
    "df_tcn2en_tcn = df_tcn2en[['product_title']].copy()\n",
    "df_tcn2en_en  = df_tcn2en[['translated_output']].copy()\n",
    "df_tcn2en_tcn = preprocess(df_tcn2en_tcn, True)\n",
    "df_tcn2en_en  = preprocess(df_tcn2en_en,  True)\n",
    "\n",
    "df_en2tcn_tcn = df_en2tcn[['translated_output']].copy()\n",
    "df_en2tcn_en  = df_en2tcn[['product_title']].copy()\n",
    "df_en2tcn_tcn = preprocess(df_en2tcn_tcn, True)\n",
    "df_en2tcn_en  = preprocess(df_en2tcn_en,  True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_tcn = df_train_tcn[df_train_tcn['product_title'].str.len() >= 2]\n",
    "df_train_en  = df_train_en[df_train_en['product_title'].str.len() >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_idx = (df_dev_tcn['product_title'].str.len() >= 2) & (df_dev_en['product_title'].str.len() >= 2)\n",
    "df_dev_tcn = df_dev_tcn[dev_idx]\n",
    "df_dev_en  = df_dev_en[dev_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = (df_test_tcn['product_title'].str.len() >= 2) & (df_test_en['product_title'].str.len() >= 2)\n",
    "df_test_tcn = df_test_tcn[test_idx]\n",
    "df_test_en  = df_test_en[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn2en_idx = (df_tcn2en_tcn['product_title'].str.len() >= 2) & (df_tcn2en_en['product_title'].str.len() >= 2)\n",
    "df_tcn2en_tcn = df_tcn2en_tcn[tcn2en_idx]\n",
    "df_tcn2en_en  = df_tcn2en_en[tcn2en_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "en2tcn_idx = (df_en2tcn_tcn['product_title'].str.len() >= 2) & (df_en2tcn_en['product_title'].str.len() >= 2)\n",
    "df_en2tcn_tcn = df_en2tcn_tcn[en2tcn_idx]\n",
    "df_en2tcn_en  = df_en2tcn_en[en2tcn_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONO [ZH,EN] ALL: 499816 499992\n"
     ]
    }
   ],
   "source": [
    "df_mono_zh_all = df_train_tcn\n",
    "df_mono_en_all = df_train_en\n",
    "print(\"MONO [ZH,EN] ALL:\", len(df_mono_zh_all), len(df_mono_en_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARA [ZH,EN] ALL: 30995 30995\n"
     ]
    }
   ],
   "source": [
    "df_para_zh_all = pd.concat([df_dev_tcn, df_test_tcn, df_tcn2en_tcn, df_en2tcn_tcn]).reset_index().drop('index', axis=1)\n",
    "df_para_en_all = pd.concat([df_dev_en , df_test_en,  df_tcn2en_en , df_en2tcn_en]).reset_index().drop('index', axis=1)\n",
    "print(\"PARA [ZH,EN] ALL:\", len(df_para_zh_all), len(df_para_en_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL [ZH,EN] ALL: 530811 530987\n"
     ]
    }
   ],
   "source": [
    "df_all_zh_all = pd.concat([df_mono_zh_all, df_para_zh_all]).reset_index().drop('index', axis=1)\n",
    "df_all_en_all = pd.concat([df_mono_en_all, df_para_en_all]).reset_index().drop('index', axis=1)\n",
    "print(\"ALL [ZH,EN] ALL:\", len(df_all_zh_all), len(df_all_en_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(df_all, alpha=0.04, beta=0.5, random_state=0):\n",
    "    df_all = df_all.sample(frac=1.0, random_state=random_state)\n",
    "    df_train, df_test = train_test_split(df_all,  test_size=alpha, random_state=random_state)\n",
    "    df_valid, df_test = train_test_split(df_test, test_size=beta,  random_state=random_state)\n",
    "    print(\"[TRAIN,VALID,TEST] : {:>6} {:>6} {:>6}\".format(len(df_train), len(df_valid), len(df_test)))\n",
    "    return df_train, df_valid, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN,VALID,TEST] : 479823   9996   9997\n",
      "[TRAIN,VALID,TEST] : 479992  10000  10000\n",
      "[TRAIN,VALID,TEST] :  27895   1550   1550\n",
      "[TRAIN,VALID,TEST] :  27895   1550   1550\n"
     ]
    }
   ],
   "source": [
    "df_mono_zh_train, df_mono_zh_valid, df_mono_zh_test = train_valid_test_split(df_mono_zh_all)\n",
    "df_mono_en_train, df_mono_en_valid, df_mono_en_test = train_valid_test_split(df_mono_en_all)\n",
    "df_para_zh_train, df_para_zh_valid, df_para_zh_test = train_valid_test_split(df_para_zh_all, 0.1)\n",
    "df_para_en_train, df_para_en_valid, df_para_en_test = train_valid_test_split(df_para_en_all, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3755</th>\n",
       "      <td>Horizontal stripes  grape mt and paper tape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25817</th>\n",
       "      <td>Ddult cotton spandex big size terno tokong for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30127</th>\n",
       "      <td>Live check out for maam chanti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17763</th>\n",
       "      <td>S-LV bag (Python-style fashion shoulder slanti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>Free shipping/thin section plus size plus size...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           product_title\n",
       "3755         Horizontal stripes  grape mt and paper tape\n",
       "25817  Ddult cotton spandex big size terno tokong for...\n",
       "30127                     Live check out for maam chanti\n",
       "17763  S-LV bag (Python-style fashion shoulder slanti...\n",
       "1418   Free shipping/thin section plus size plus size..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_para_en_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3755</th>\n",
       "      <td>横纹・葡萄 mt和纸胶带</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25817</th>\n",
       "      <td>女士Ddult棉氨纶大号Terno Tokong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30127</th>\n",
       "      <td>现场检查Maam Chanti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17763</th>\n",
       "      <td>S-LV 包包（蟒蛇纹时尚手提单肩斜跨包）「认明Yuanroro优质</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>全馆免运/薄款加大码女宽松破洞牛仔短裤 YX041729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            product_title\n",
       "3755                         横纹・葡萄 mt和纸胶带\n",
       "25817            女士Ddult棉氨纶大号Terno Tokong\n",
       "30127                     现场检查Maam Chanti\n",
       "17763  S-LV 包包（蟒蛇纹时尚手提单肩斜跨包）「认明Yuanroro优质\n",
       "1418         全馆免运/薄款加大码女宽松破洞牛仔短裤 YX041729"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_para_zh_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save preprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mono_zh_all.to_csv(  'mass/all.zh',        header=False, index=False)\n",
    "df_mono_en_all.to_csv(  'mass/all.en',        header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mono_zh_train.to_csv('mass/mono/train.zh', header=False, index=False)\n",
    "df_mono_en_train.to_csv('mass/mono/train.en', header=False, index=False)\n",
    "df_mono_zh_valid.to_csv('mass/mono/valid.zh', header=False, index=False)\n",
    "df_mono_en_valid.to_csv('mass/mono/valid.en', header=False, index=False)\n",
    "df_mono_zh_test.to_csv( 'mass/mono/test.zh',  header=False, index=False)\n",
    "df_mono_en_test.to_csv( 'mass/mono/test.en',  header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_para_zh_train.to_csv('mass/para/train.zh', header=False, index=False)\n",
    "df_para_en_train.to_csv('mass/para/train.en', header=False, index=False)\n",
    "df_para_zh_valid.to_csv('mass/para/valid.zh', header=False, index=False)\n",
    "df_para_en_valid.to_csv('mass/para/valid.en', header=False, index=False)\n",
    "df_para_zh_test.to_csv( 'mass/para/test.zh',  header=False, index=False)\n",
    "df_para_en_test.to_csv( 'mass/para/test.en',  header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print((df_para_zh_train['product_title'].str.len() == 1).sum())\n",
    "print((df_mono_en_train['product_title'].str.len() == 1).sum())\n",
    "print((df_para_zh_train['product_title'].str.len() == 1).sum())\n",
    "print((df_para_en_train['product_title'].str.len() == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mono_zh_all.to_csv(  \"product-translation-dataset/clean/mono_zh_all.csv\", index=False)\n",
    "df_mono_en_all.to_csv(  \"product-translation-dataset/clean/mono_en_all.csv\", index=False)\n",
    "df_mono_zh_train.to_csv(\"product-translation-dataset/clean/mono_zh_train.csv\", index=False)\n",
    "df_mono_en_train.to_csv(\"product-translation-dataset/clean/mono_en_train.csv\", index=False)\n",
    "df_mono_zh_valid.to_csv(\"product-translation-dataset/clean/mono_zh_valid.csv\", index=False)\n",
    "df_mono_en_valid.to_csv(\"product-translation-dataset/clean/mono_en_valid.csv\", index=False)\n",
    "df_mono_zh_test.to_csv( \"product-translation-dataset/clean/mono_zh_test.csv\", index=False)\n",
    "df_mono_en_test.to_csv( \"product-translation-dataset/clean/mono_en_test.csv\", index=False)\n",
    "df_para_zh_valid.to_csv(\"product-translation-dataset/clean/para_zh_valid.csv\", index=False)\n",
    "df_para_en_valid.to_csv(\"product-translation-dataset/clean/para_en_valid.csv\", index=False)\n",
    "df_para_zh_test.to_csv( \"product-translation-dataset/clean/para_zh_test.csv\", index=False)\n",
    "df_para_en_test.to_csv( \"product-translation-dataset/clean/para_en_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_clean = pd.concat(\n",
    "    [df_mono_zh_all,   df_mono_en_all,\n",
    "     df_mono_zh_train, df_mono_en_train,\n",
    "     df_mono_zh_valid, df_mono_en_valid,\n",
    "     df_mono_zh_test,  df_mono_en_test,\n",
    "     df_para_zh_valid, df_para_en_valid, \n",
    "     df_para_zh_test,  df_para_en_test\n",
    "    ]\n",
    ").reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_clean.to_csv(\"product-translation-dataset/clean/all_clean.csv\", index=False)\n",
    "df_train_tcn.to_csv(\"product-translation-dataset/clean/train_zh_clean.csv\", index=False)\n",
    "df_train_en.to_csv( \"product-translation-dataset/clean/train_en_clean.csv\", index=False)\n",
    "df_test_tcn.to_csv( \"product-translation-dataset/clean/test_zh_clean.csv\", index=False)\n",
    "df_dev_tcn.to_csv(  \"product-translation-dataset/clean/dev_zh_clean.csv\", index=False)\n",
    "df_dev_en.to_csv(   \"product-translation-dataset/clean/dev_en_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_dataset(df, lines_per_part=15000):\n",
    "    df = df.sample(frac=1.0)\n",
    "    partitions = []\n",
    "    for g, df_part in df.groupby(np.arange(len(df)) // lines_per_part):\n",
    "        partitions.append(df_part)\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tcn_chunks = partition_dataset(df_train_tcn, 10000)\n",
    "df_en_chunks  = partition_dataset(df_train_en,  10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(df_tcn_chunks):\n",
    "    df.to_csv(\"product-translation-dataset/train_tcn_chunks/part_{}.csv\".format(i+1), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(df_en_chunks):\n",
    "    df.to_csv(\"product-translation-dataset/train_en_chunks/part_{}.csv\".format(i+1),  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tcn_chunk_1 = pd.read_csv('product-translation-dataset/train_tcn_chunks/part_1.csv')\n",
    "df_en_chunk_1 = pd.read_csv('product-translation-dataset/train_en_chunks/part_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324372"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(df_tcn_chunk_1['product_title'].str.len().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370912"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(df_en_chunk_1['product_title'].str.len().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "329277 + 372034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
