{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing import image as tfk_image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator as tfk_ImageDataGenerator\n",
    "from tensorflow.python.keras.applications.efficientnet import EfficientNetB7 as tfk_BaseModel\n",
    "from tensorflow.python.keras.applications.efficientnet import preprocess_input as tfk_preprocess_input\n",
    "\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set();\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix, log_loss, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fd663cf2b6e1d7b02938c6aaae0a32d2.jpg</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c7fd77508a8c355eaab0d4e10efd6b15.jpg</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127f3e6d6e3491b2459812353f33a913.jpg</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5ca4f2da11eda083064e6c36f37eeb81.jpg</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46d681a542f2c71be017eef6aae23313.jpg</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               filename  category\n",
       "0  fd663cf2b6e1d7b02938c6aaae0a32d2.jpg        43\n",
       "1  c7fd77508a8c355eaab0d4e10efd6b15.jpg        43\n",
       "2  127f3e6d6e3491b2459812353f33a913.jpg        43\n",
       "3  5ca4f2da11eda083064e6c36f37eeb81.jpg        43\n",
       "4  46d681a542f2c71be017eef6aae23313.jpg        43"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_test = pd.read_csv('shopee-product-detection-dataset/test.csv')\n",
    "pd_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "input_shape=(224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12186 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = tfk_ImageDataGenerator(\n",
    "    preprocessing_function=tfk_preprocess_input\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    pd_test,\n",
    "    directory='shopee-product-detection-dataset/test/',\n",
    "    x_col='filename',\n",
    "    target_size=input_shape[:2],\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gcloud-models/dict.txt', 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"gcloud-models/model.tflite\")\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.resize_tensor_input(input_details[0]['index'], [batch_size, *input_shape])\n",
    "interpreter.resize_tensor_input(output_details[0]['index'],[batch_size, *input_shape])\n",
    "\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num steps: 12187\n",
      "step: 100 33.054864168167114\n",
      "step: 200 33.55562114715576\n",
      "step: 300 32.36565017700195\n",
      "step: 400 31.918362140655518\n",
      "step: 500 31.835959672927856\n",
      "step: 600 32.1205530166626\n",
      "step: 700 32.15298128128052\n",
      "step: 800 34.4155809879303\n",
      "step: 900 31.622830152511597\n",
      "step: 1000 31.811530113220215\n",
      "step: 1100 32.67298245429993\n",
      "step: 1200 34.17414712905884\n",
      "step: 1300 32.8804292678833\n",
      "step: 1400 31.9122257232666\n",
      "step: 1500 32.133999824523926\n",
      "step: 1600 32.498602628707886\n",
      "step: 1700 33.042184591293335\n",
      "step: 1800 33.724451303482056\n",
      "step: 1900 34.17407464981079\n",
      "step: 2000 33.19287347793579\n",
      "step: 2100 32.47928190231323\n",
      "step: 2200 32.043450355529785\n",
      "step: 2300 33.33682203292847\n",
      "step: 2400 32.96821618080139\n",
      "step: 2500 32.50149607658386\n",
      "step: 2600 32.5614857673645\n",
      "step: 2700 32.217424154281616\n",
      "step: 2800 32.50444006919861\n",
      "step: 2900 31.78349280357361\n",
      "step: 3000 33.27624154090881\n",
      "step: 3100 33.207152366638184\n",
      "step: 3200 31.9429931640625\n",
      "step: 3300 32.05816698074341\n",
      "step: 3400 32.06964111328125\n",
      "step: 3500 32.05737829208374\n",
      "step: 3600 33.43335008621216\n",
      "step: 3700 32.46531105041504\n",
      "step: 3800 32.570059299468994\n",
      "step: 3900 32.54370927810669\n",
      "step: 4000 31.79638981819153\n",
      "step: 4100 32.16844296455383\n",
      "step: 4200 32.935856103897095\n",
      "step: 4300 31.663423538208008\n",
      "step: 4400 31.589025259017944\n",
      "step: 4500 31.74679660797119\n",
      "step: 4600 32.441433906555176\n",
      "step: 4700 32.0958993434906\n",
      "step: 4800 31.54711651802063\n",
      "step: 4900 31.613224506378174\n",
      "step: 5000 31.5732741355896\n",
      "step: 5100 31.80293297767639\n",
      "step: 5200 32.15201759338379\n",
      "step: 5300 32.924784660339355\n",
      "step: 5400 33.86608862876892\n",
      "step: 5500 34.41298699378967\n",
      "step: 5600 32.05450940132141\n",
      "step: 5700 31.778754234313965\n",
      "step: 5800 33.42552447319031\n",
      "step: 5900 32.0980589389801\n",
      "step: 6000 32.37261414527893\n",
      "step: 6100 32.94110131263733\n",
      "step: 6200 31.524006843566895\n",
      "step: 6300 31.539270401000977\n",
      "step: 6400 31.57782745361328\n",
      "step: 6500 31.879164695739746\n",
      "step: 6600 32.95350503921509\n",
      "step: 6700 32.689534425735474\n",
      "step: 6800 32.06519627571106\n",
      "step: 6900 32.43919801712036\n",
      "step: 7000 32.262662410736084\n",
      "step: 7100 32.49879240989685\n",
      "step: 7200 32.15499305725098\n",
      "step: 7300 32.71255087852478\n",
      "step: 7400 32.04426980018616\n",
      "step: 7500 32.19111466407776\n",
      "step: 7600 32.99237823486328\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = []\n",
    "\n",
    "num_steps = test_generator.n//batch_size+1\n",
    "print(\"num steps:\", num_steps)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(num_steps):\n",
    "  if (i+1) % 100 == 0:\n",
    "    print(\"step:\", i+1, time.time() - start)\n",
    "    start = time.time()\n",
    "  \n",
    "  x = next(iter(test_generator))\n",
    "  x = x.astype(np.uint8)\n",
    "  \n",
    "  interpreter.set_tensor(input_details[0]['index'], x)\n",
    "  interpreter.invoke()\n",
    "  output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "  output_labels = np.asarray([int(classes[i]) for i in output_data.argmax(axis=1)])\n",
    "  y_pred.extend(output_labels)\n",
    "\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
